1、在下采样操作中尽可能使用步长为2的操作代替池化。

在多通道网络的相关论文中阐述过一个关于卷积网络缺陷的例子。一个训练好的卷积网络只能根据局部特征来处理比较接近训练数据集的图像，在处理异常图像时，比如颠倒、倾斜、其他朝向的相关图像，卷积神经网络则会表现得很差。

相关论文可在arXiv网站中搜索论文编号"1710.09829"。

造成这种现象的原因是，卷积神经网络中的池化操作弄丢了一些隐含信息，这使得它只能发现局部组建的特征，不能发现组建之间的定向关系和相对空间关系。

在卷积神经网络中，池化操作可以让局部特征更明显，但在提升局部特征的同时也弄丢了其内在的其他信息，如位置信息。如果在所处理的任务中包含组建间的位置关系，则在所搭建的卷积神经网络结构中尽量不要使用池化操作，可以在网络中将下采样行为由池化操作转换为卷积运算。常用的下采样操作会将尺寸缩小一半，对于这种情况，可以使用步长为2的卷积。

例如，YOLOV3模型中的Darknet-53模型使用的就是这种技术，另外，在EfficientNet模型的输出层中，也将最大池化换成了步长为2的卷积运算。



2、批量归一化与激活函数的位置关系。

批量归一化层(Batch Normalization)和激活函数的先后顺序在不同情况下会不同。批量归一化与激活函数的前后关系，本质上还是值域间的变换关系，由于不同的激活函数有不同的值域，因此不能一概而论。

针对sigmoid激活函数，当x值大于7.5或者小于-7.5时，在直角坐标系中，所对应的y值几乎不变，这表明sigmoid激活函数对过大或过小的数无法产生激活作用，这种令sigmoid激活函数失效的区间叫做sigmoid的饱和区间。这种情况下，当前层网络输出全是1或-1，下一层网络将无法在对全1或-1的特征数据进行计算，导致模型在训练中无法收敛。

如果网络中有类似sigmoid这种带饱和区间的激活函数，则应该将BN层放在激活函数的前面，这样经过BN处理后的特征数据值域就变成了-1到1，再输入激活函数中，便可以正常实现非线性转换的功能。

而对于relu激活函数，应该将BN层放在relu层之后，这样不会对数据的正负比例造成影响，在保证正负比例的基础上再执行BN操作，可以使效果达到最优。


实验效果：
（1）BN放在relu前面： acc=0.474；
（2）BN放在relu前面，后面还有缩放和偏差：acc=0.478；
（3）BN放在relu后面：acc=0.499；
（4）BN放在relu后面，后面还有缩放和偏差：acc=0.493。



3、应该将图片归一化到[0，1]区间还是[-1，1]区间？

在用神经网络处理图片时，必要的步骤就是图片的预处理。在预处理过程中，有的模型会将图片归一化到[0，1]区间，而有的模型会将图片归一化到[-1，1]区间，实际中应该如何选择呢？

实验可以，归一化到[-1，1]区间的模型收敛速度会稍微慢一些，val loss更高，由此可以看出，将图片归一化到[0，1]区间的效果要好于[-1，1]区间，这是有原因的。

图片要归一化的值域与图片本身的值域特点有关，一般情况下，图片处理过程中的值域，一部分来自于图片本身，另一部分来自于填充值。我们在向模型输入图片数据时，先对图片尺寸做了同比例缩放，然后又向图片中填充了0。填充值应该遵循最少地改变原有数据分布的原则，要使填充值对特征运算的影响最小，一般会取原有数据的下限。

正是由于填充值的影响，才使得将图片归一化到[0，1]区间要好于归一化到[-1，1]区间。如果用0进行填充，并且把图片归一化到[-1，1]区间，则会在原始图片中加入很多中间值，这会影响原始的分布。

总结起来，将图片归一化到[0，1]区间或[-1，1]区间并没有太大的区别，在选择时重点要与程序的其他部分结合起来。因为本例使用0作为填充值，所以将图片归一化到[0，1]区间；如果要将图片归一化到[-1，1]区间，应该将填充值设置为-1。



4、训练时优化器的选取：

Amsgrad优化器综合性能优于Adam，实验效果如下：模型在经过400次迭代训练后，adam输出的val loss = 0.9148，远大于Amsgrad优化器的val loss = 0.7458。

SGD优化器对学习率比较挑剔。SGD是一个对学习率大小非常敏感的优化器，一旦设置的学习率不合适，训练出来的效果会很差。例如此处设置lr = 0.02训练400轮，val loss = 41.5003。
