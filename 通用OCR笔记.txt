1、OCR的本质是一个跨域转换任务，即把图像的特征转换为文本的序列特征。在实现时，一般会采用两阶段方式进行。

（1）检测文字。在图片上搜索含文字的区域，并将该区域的内容提取出来。
（2）识别文字。对文字检测阶段所提取出来的子图进行处理，将其转换为具体的文字。

其中文字检测阶段与目标检测完全一致，可以直接使用目标检测模型进行实现；文字识别阶段实现了跨域转换，一般会先使用卷积网络对图片进行特征处理，再将特征转换为序列文字。


2、数据集下载： https://blog.csdn.net/weixin_45779880/article/details/105642393，但质量不佳。


3、连接时序分类算法：Connectionist Temporal Classification，CTC。

CTC算法是基于神经网络的时序类分类技术，常用来解决输入序列和输出序列难以一一对应的问题。CTC算法主要用于处理损失值，即通过对序列未对齐的标签添加空标签，将预测的输出值与给定的标签值在时间序列上对齐，通过交叉熵算法求出具体损失值。

在训练过程中，CTC算法增加了一个额外的符号以代表序列中的空位，并通过递推方式来快速计算梯度。在预测过程中，CTC算法使用贪心搜索算法，从输出结果中找出最适合的一条序列作为最终结果。


4、对于图片大小和文字长度并不固定的OCR情形，需要对每张图片根据自身的宽高比进行缩放，然后将不足的区域用黑色填充，从而使得它们的长度相等，文字大小尺寸也近似相同。


5、crnn模型在对卷积运算的步长上进行了调整，使得整个模型在高度方向上进行了5次下采样，在宽度方向上进行了2次下采样。经过该变换之后，feature map的高度是原始图片的1/32，宽度是1/4。输入图片尺寸为：(None, 64, 128, 3)，提取得到的特征图尺寸为：(None, 2, 32, 256)。

Permute层: 根据给定的dim置换输入的维度, 根据指定的模式重新排列。尺寸由(None, 2, 32, 256)转变为(None, 32, 2, 256)。
TimeDistributed函数: 实现了输入数据从三维到二维的转换，将输入数据的最后两个维度展开，其实本质上就是将最后两个维度所有数据拉直合并。原本是(None, 32, 2, 256)，现在拉直变成了(None, 32, 512)。

函数TimeDistribute的本意是沿着时间维度进行操作，该函数里面的嵌套函数Flatten的作用才是使维度展开，利用Reshape函数也可以的达到相同的效果，作用相同。


6、函数CuDNNGRU的参数介绍如下：

第一个参数128代表单元的个数，第二个参数return_sequence=True代表单元将输出每个时刻的处理结果。如果该值为False，则只输出最后一个时刻的处理结果。

经过双向RNN的处理后，输出的形状为(None, 32, 256)，其中32代表序列长度为32，256代表每个cell都是一个向量，由正反两个方向各128个单元所输出的结果组成。


7、Amsgrad优化器，在训练过程中，该优化器使用了二阶动量进行优化，其效果优于Adam优化器，该优化器来自一篇名为"On the Convergence of Adam and Beyond"的论文。


8、拿crnn模型识别验证码，训练效果令我大跌眼镜......

（1）train loss能降到很低，但val loss会出现瓶颈，到达0.9就降不下去了。
（2）更让我大跌眼镜的是，训练过程中val loss反复横跳，有时从0.8突然跳到5.6，而且经常不稳定。
（3）测试集精度最后是0%，我不知道咋会训练出这种效果......


我好像找到bug了，模型搭建时有个逻辑bug：

input_length = np.ones((self.batch_size, 1)) * (img_w // 8 - 2)      # (batch, 1)

这里不应该除以8，而应该128//4=32， 32-2=30，因为我整个rnn序列长度为32，所以应该用img_w长度除以2的2次方，两次下采样。


9、意外学会了去除pycharm中多余的conda环境的方法，直接用左下角的“减号”即可。

10、top命令：查看CPU使用率，然后按数字1，显示每个CPU使用率。


11、修改完上面那个逻辑bug之后，算法效果极佳，真不愧是石破天惊的rnn序列神经网络！！！

之前那个CNN序列分类效果只能达到65%，而此处同样的数据集，训练10轮之后序列识别精度能达到97.3%，超强！！！

而且在误判图片里面，那些图片的字符起码都是连肉眼都难以区分出来的。

